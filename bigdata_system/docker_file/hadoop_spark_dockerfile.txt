FROM ubuntu:22.04
RUN ln -snf /usr/share/zoneinfo/Asia/Seoul /etc/localtime
RUN apt update --fix-missing 
RUN apt upgrade -y
#RUN apt update
RUN apt install -y openssh-server
RUN apt install -y openssh-client
RUN apt install -y pdsh
RUN apt-get install -y vim

RUN apt install less
RUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/gi' /etc/ssh/sshd_config

RUN ssh-keygen -t rsa -N '' -f /root/.ssh/id_rsa 
RUN cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
RUN chmod 0600 ~/.ssh/authorized_keys
RUN echo "ssh" > /etc/pdsh/rcmd_default
RUN apt install -y openjdk-8-jdk
RUN mkdir /root/hadoop
RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz -P /root/hadoop
RUN tar zxvf /root/hadoop/hadoop-3.3.4.tar.gz -C /root/hadoop --strip 1
RUN echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64" >> /root/hadoop/etc/hadoop/hadoop-env.sh
RUN sed -i '20d' /root/hadoop/etc/hadoop/core-site.xml
RUN printf  "<property>\n\
             <name>fs.defaultFS</name>\n\
             <value>hdfs://localhost:9000</value>\n\
             </property>\n\
            <property>\n\
             <name>dfs.replication</name>\n\
             <value>1</value>\n\
          </property>\n\
          </configuration>" >> /root/hadoop/etc/hadoop/core-site.xml




RUN sed -i '21d' /root/hadoop/etc/hadoop/mapred-site.xml

RUN printf  "<property>\n\
            <name>mapreduce.framework.name</name>\n\
              <value>yarn</value>\n\
        </property>\n\
        <property>\n\
            <name>yarn.app.mapreduce.am.env</name>\n\
            <value>HADOOP_MAPRED_HOME=/root/hadoop</value>\n\
        </property>\n\
        <property>\n\
            <name>mapreduce.map.env</name>\n\
             <value>HADOOP_MAPRED_HOME=/root/hadoop</value>\n\
        </property>\n\
        <property>\n\
            <name>mapreduce.reduce.env</name>\n\
            <value>HADOOP_MAPRED_HOME=/root/hadoop</value>\n\
        </property></configuration>" >> /root/hadoop/etc/hadoop/mapred-site.xml

RUN sed -i '19d' /root/hadoop/etc/hadoop/yarn-site.xml
RUN printf  "<property>\n\
                <name>yarn.nodemanager.aux-services</name>\n\
                <value>mapreduce_shuffle</value>\n\
            </property>\n\
            <property>\n\
                <name>yarn.nodemanager.vmem-check-enabled</name>\n\
                <value>false</value>\n\
            </property>\n\
            </configuration>\n"  >> /root/hadoop/etc/hadoop/yarn-site.xml

RUN apt install less

RUN /bin/bash -c  "apt-get install python3-pip -y"


#한글 다운로드
RUN apt-get install locales

RUN apt-get install unzip

RUN apt-get install expect -qy

RUN apt-get update
RUN apt-get install git -y


RUN mkdir /root/elastic_hadoop/
RUN wget https://artifacts.elastic.co/downloads/elasticsearch-hadoop/elasticsearch-hadoop-7.17.13.zip -P /root/
RUN unzip /root/elasticsearch-hadoop-7.17.13.zip   -d /root
RUN cp 	/root/elasticsearch-hadoop-7.17.13/dist/*  /root/elastic_hadoop/
RUN rm /root/elasticsearch-hadoop-7.17.13.zip
RUN rm -r /root/elasticsearch-hadoop-7.17.13



#=========== elastic search 설치 ====================================
RUN apt update	--fix-missing 
RUN apt install curl -y 
RUN apt-get install net-tools -y 
RUN apt install apt-transport-https ca-certificates wget  -y --no-install-recommends 
RUN curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -
RUN echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-7.x.list

RUN apt update	--fix-missing 
RUN apt install elasticsearch -y --no-install-recommends 

RUN printf "\n\
cluster.name: my-application\n\
node.name: node-1\n\
node.master: true\n\
node.data: true\n\
network.host: 0.0.0.0\n\
http.port: 9201\n\
transport.tcp.port: 9301\n\
discovery.seed_hosts: [\"127.0.0.1:9301\"]\n\
cluster.initial_master_nodes: [\"node-1\"]\n\
discovery.zen.minimum_master_nodes: 1"   >> /etc/elasticsearch/elasticsearch.yml

RUN wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -
RUN apt-get install apt-transport-https -y --no-install-recommends  
RUN echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | tee /etc/apt/sources.list.d/elastic-7.x.list
RUN  apt-get update 
RUN apt-get install kibana  -y --no-install-recommends
RUN apt-get install logstash -y --no-install-recommends

RUN printf "\n\
server.port: 5601\n\
server.host: \"0.0.0.0\"\n\
elasticsearch.hosts: [\"http://localhost:9201\"]" >>/etc/kibana/kibana.yml


#======================= spark  설치 =========================
RUN mkdir /root/spark
RUN wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz -P /root/spark
RUN tar -xzvf /root/spark/spark-3.2.1-bin-hadoop3.2.tgz -C /root/spark --strip 1
RUN pip3 install  pyspark findspark

RUN cp /root/spark/conf/spark-env.sh.template /root/spark/conf/spark-env.sh

RUN printf "\n\
export SPARK_HOME=/root/spark\n\
export SPARK_CONF_DIR=/root/spark/conf\n\
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\
export HADOOP_HOME=/root/hadoop\n\
export HADOOP_CONF_DIR=/root/hadoop/etc/hadoop" >> /root/spark/conf/spark-env.sh

RUN cp /root/spark/conf/spark-defaults.conf.template /root/spark/conf/spark-defaults.conf

RUN printf "\n\
spark.master              yarn \n\
spark.eventLog.enabled    true \n\
spark.eventLog.dir        /root/spark/logs " >> /root/spark/conf/spark-defaults.conf

RUN mkdir  /root/spark/logs

RUN mkdir /root/zeppelin
RUN wget https://dlcdn.apache.org/zeppelin/zeppelin-0.10.1/zeppelin-0.10.1-bin-all.tgz -P /root/zeppelin
RUN tar -zxvf /root/zeppelin/zeppelin-0.10.1-bin-all.tgz -C /root/zeppelin --strip 1

RUN chmod -R 777 /root/zeppelin
RUN chmod -R 777 /root/spark

RUN cp /root/zeppelin/conf/zeppelin-site.xml.template /root/zeppelin/conf/zeppelin-site.xml
RUN sed -i 's/127\.0\.0\.1/0\.0\.0\.0/gi' /root/zeppelin/conf/zeppelin-site.xml
RUN sed -i 's/8080/8877/gi' /root/zeppelin/conf/zeppelin-site.xml

RUN cp /root/zeppelin/conf/zeppelin-env.sh.template /root/zeppelin/conf/zeppelin-env.sh

#========= 설정 =====================================================

RUN printf "\n\
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\
export HADOOP_HOME=/root/hadoop\n\
export YARN_CONF_DIR=\$HADOOP_HOME/etc/hadoop \n\
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop \n\
export SPARK_HOME=/root/spark \n\
export SPARK_MASTER=yarn \n\
export ZEPPELIN_PORT=8877 \n\
export PYTHONPATH=/usr/bin/python3 \n\
export PYSPARK_PYTHON=/usr/bin/python3 \n\
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3" >> /root/zeppelin/conf/zeppelin-env.sh


RUN printf "\n\
export HADOOP_HOME=/root/hadoop \n\ 
export HADOOP_MAPRED_HOME=\$HADOOP_HOME\n\
export HADOOP_COMMON_HOME=\$HADOOP_HOME\n\
export HADOOP_HDFS_HOME=\$HADOOP_HOME\n\
export YARN_HOME=\$HADOOP_HOME\n\
export HDFS_NAMENODE_USER=\"root\"\n\
export HDFS_DATANODE_USER=\"root\"\n\
export HDFS_SECONDARYNAMENODE_USER=\"root\"\n\
export YARN_RESOURCEMANAGER_USER=\"root\"\n\
export YARN_NODEMANAGER_USER=\"root\"\n\
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\
HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop\n\
export JAVA_HOME HADOOP_CONF_DIR\n\
export LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:\$HADOOP_HOME/lib/native\n\
export SPARK_HOME=/root/spark\n\
export PYTHONPATH=/usr/bin/python3\n\
export PYSPARK_PYTHON=/usr/bin/python3\n\
export ZEPPELIN_HOME=/root/zeppelin\n\
export PATH=\$PATH:/usr/bin/python3:/root:/root/zeppelin/bin:\$SPARK_HOME/bin:\$SPARK_HOME/sbin:\$JAVA_HOME/bin:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin">> /root/.bashrc

RUN echo "202310131206"

#https://drive.google.com/file/d/1xAJefePJcdk0uoOvK5qTkBXkTR6OqZMZ/view?usp=drive_link
RUN wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xAJefePJcdk0uoOvK5qTkBXkTR6OqZMZ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1xAJefePJcdk0uoOvK5qTkBXkTR6OqZMZ" -O /root/spark_start.sh && rm -rf /tmp/cookies.txt

RUN  sed -i 's/\r$//' /root/spark_start.sh 
RUN chmod 777 /root/spark_start.sh

RUN pip3 install geopy==1.17.0
RUN pip3 install numpy
RUN pip3 install pandas
RUN pip3 install matplotlib
RUN pip3 install seaborn
RUN pip3 install joblib
RUN pip3 install missingno
RUN pip3 install dtreeviz[pyspark]
RUN apt-get install graphviz -y

WORKDIR /root