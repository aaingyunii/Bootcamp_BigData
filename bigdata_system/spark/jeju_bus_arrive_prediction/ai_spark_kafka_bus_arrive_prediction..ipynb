{
  "metadata": {
    "name": "ai_spark_kafka_bus_arrive_prediction",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip uninstall pandas -y"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install -U pandas\u003d\u003d1.5.3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install ksql"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- 기존의 pandas 버전과 ksql이 충돌나는 것으로 보여\n- pandas 다른 버전을 다운받아서 진행"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom ksql import KSQLAPI\nimport pandas as pd\nimport time\nimport datetime\nimport numpy as np\nfrom pyspark.ml.regression import RandomForestRegressionModel\nfrom pyspark.ml.feature import VectorAssembler\nimport ast"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nclient \u003d KSQLAPI(\"http://localhost:8089\", timeout\u003dNone)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 하둡에 저장된 모델 읽어서 저장\nload_rf \u003d RandomForestRegressionModel.load(\"/spark_rf_model/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nload_rf"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeature_name_list \u003d [\n                    \u0027now_latitude\u0027, \u0027now_longitude\u0027, \u0027now_arrive_time\u0027,\n                    \u0027distance\u0027,\u0027next_latitude\u0027,\u0027next_longitude\u0027,\u0027weekday\u0027\n                    ]"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeature_name_list"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 실시간 버스 위치가 저장된 테이블의 모든 레코드 조회하는 객체 생성\n\nget_change_table \u003d client.query(\"\"\"SELECT * FROM bus_location_topic EMIT CHANGES;\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# bus_location_topic에 추가된 데이터가 있을 때까지 대기하고 있다\n## 추가된 데이터가 있으면 반복문 실행\n\nfor i in get_change_table:\n    print(f\"수정된 테이블 조회 : {i}\")\n    \n    if \"row\" in i:\n        location \u003d ast.literal_eval(i)[0][\"row\"][\"columns\"]\n        print(f\"위치 : {location}\")\n        \n        id \u003d location[0]\n        date \u003d location[1]\n        route_id \u003d location[2]\n        vh_id \u003d location[3]\n        route_nm \u003d location[4]\n        now_latitude \u003d location[5]\n        now_longitude \u003d location[6]\n        now_station \u003d location[7]\n        now_arrive_time \u003d location[8]\n        distance \u003d location[9]\n        next_station \u003d location[10]\n        next_latitude \u003d location[11]\n        next_longitude \u003d location[12]\n        weekday \u003d location[13]\n        \n        # 예측을 위해 버스 위치 정보를 Pandas DataFrame 으로 생성\n        df_location \u003d pd.DataFrame({\n            \"now_latitude\" : [now_latitude],\n            \"now_longitude\" : [now_longitude],\n            \"now_arrive_time\" : [now_arrive_time],\n            \"distance\" : [distance],\n            \"next_latitude\" : [next_latitude],\n            \"next_longitude\" : [next_longitude],\n            \"weekday\" : [weekday]\n        })\n        \n        # Pandas DataFrame인 df_location을 Spark DataFrame 으로 변환\n        # Spark RandomForest 모델에서 예측을 위해\n        df_spark \u003d spark.createDataFrame(df_location)\n        \n        assembler \u003d VectorAssembler(inputCols\u003dfeature_name_list, outputCol\u003d\"features\")\n        \n        assembler_df \u003d assembler.transform(df_spark)\n        \n        prediction \u003d load_rf.transform(assembler_df)\n        \n        arrive_time \u003d prediction.toPandas().loc[0,\"prediction\"]\n        print(\"#\"*100)\n\n        # 학습시 예측을 로그 값으로 예측하기 때문에 np.exp(로그값) \u003d\u003e 원래 값으로 변환 출력\n        print(f\"버스로 전송할 도착 예정 시간 : {np.exp(arrive_time)}\")\n        print(\"#\"*100)\n        \n        # 다음 정류장에 도착 예정 시간을 소수 3번째 자리 반올림\n        next_arrive_time \u003d round(np.exp(arrive_time),3)\n        \n        insert_query \u003d f\"\"\"INSERT INTO bus_arrive_topic (\n                id\n                ,route_id\n                ,vh_id\n                ,route_nm\n                ,now_latitude\n                ,now_longitude\n                ,now_station\n                ,now_arrive_time\n                ,distance\n                ,next_station\n                ,next_latitude\n                ,next_longitude\n                ,weekday\n                ,date\n                ,next_arrive_time\n                ) VALUES ({id},\u0027{route_id}\u0027,\u0027{vh_id}\u0027\n                        ,\u0027{route_nm}\u0027,{now_latitude}\n                        ,{now_longitude},\u0027{now_station}\u0027\n                        ,{now_arrive_time},{distance}\n                        ,\u0027{next_station}\u0027,{next_latitude}\n                        ,{next_longitude},{weekday}\n                        ,\u0027{date}\u0027,{next_arrive_time}\n                        );\"\"\"\n        print(f\"insert query :\\n{insert_query}\")\n\n        try:\n            client.ksql(insert_query) # KSQLDB에 현재 위치 추가\n        except Exception as e:\n            print(f\"Exception : \\n{e}\")\n            \n    print(\"\u003d\"*100)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}