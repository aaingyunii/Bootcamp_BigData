{
  "metadata": {
    "name": "spark_bus_ml",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import col,isnan, when, count\nfrom pyspark.sql.types import IntegerType, DoubleType\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import translate\nfrom pyspark.sql.functions import log1p\nfrom pyspark.sql.functions import dayofweek\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.sql.functions import exp\nimport joblib\n\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import lit\n\n#그래프의 모양을 설정\nplt.style.use(\u0027ggplot\u0027)\n#글자 크기 설정 \nsns.set(font_scale\u003d2) \n\n#경고 메시지가 출력되지 않도록 설정\nimport warnings\nwarnings.filterwarnings(\u0027ignore\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 하둡 data 디렉토리의 bus_train.csv 를 읽어 들임\ndf_train \u003d spark.read.csv(\"/data/bus_train.csv\",  header\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# df_train에 저장된 데이터 출력\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# csv 파일을 읽어 들인 df_train 의 타입은 spark 의 DataFrame\ntype(df_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#translate(컬럼명, 원래문자, 바꿀 문자)\n#translate(\u0027now_arrive_time\u0027, \u0027시\u0027, \u0027\u0027) : now_arrive_time 컬럼에서 \u0027시\u0027 를 삭제 \ndf_train \u003d df_train.withColumn(\u0027now_arrive_time\u0027, translate(\u0027now_arrive_time\u0027, \u0027시\u0027, \u0027\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# df_train 의 컬럼 타입 조회\ndf_train.schema"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#데이터 프레임의 컬럼 이름 조회\ndf_train.schema.names"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ncolumn_name_list \u003d df_train.schema.names"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 전체 컬럼명에서 문자열이 저장된 컬럼명 \"date\",\u0027now_station\u0027, \u0027next_station\u0027 제거한 리스트 리턴\nnumeric_column_name_list \u003d list(\n                    set(column_name_list) \n                    - set([\n                            \"id\",\"route_id\",\"vh_id\",\n                            \"date\",\u0027now_station\u0027, \n                            \u0027next_station\u0027, \"route_nm\"\n                            ]\n                          )\n                    )"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nnumeric_column_name_list"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# double 숫자 타입으로 변환\n\nfor i in  numeric_column_name_list:\n    print(\"column_name \u003d \", i)\n    df_train \u003d df_train.withColumn(i, df_train[i].cast(\u0027double\u0027))\n    print(\"\u003d\" * 100)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# df_train 의 컬럼 타입 조회\ndf_train.schema"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#df_train.columns : df_train의 컬럼 이름을 조회\n\n#for column_name in df_train.columns: df_train의 컬럼 이름을 column_name에 대입\n\n#count(column_name) : 컬럼의 데이터 수 조회(결측치 제외) \n\n# df_train.select 데이터 조회\n\n\ndf_train.select([count(column_name) for column_name in df_train.columns] ).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport missingno as msno\n#msno.bar는 각 컬럼의 null 이 아닌 데이터의 비율을 그래프로 그려주는 패키지\n# df_train.toPandas() : Spark DataFrame인 df_train 을 Pandas DataFrame으로 변환 \nmsno.bar(df\u003ddf_train.toPandas())"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# SQL 사용을 위해서 Spark DataFrame을 View(데이터를 추가 수정 할 수 없는 테이블) 형태로 변환\ndf_train.createOrReplaceTempView(\u0027bus_view\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n--데이터 조회\nselect * from bus_view"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n-- 운행시간 (now_arrive_time), 데이터수 (count(*) ) 조회\n\nselect now_arrive_time, count(*)  -- 운행시간 (now_arrive_time), 데이터수 (count(*) ) 조회\nfrom bus_view \ngroup by now_arrive_time   -- 운행 시간별 그룹\norder by now_arrive_time   -- 운행 시간으로 정렬"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n-- now_station(정류소명), now_longitude(위도), now_latitude (경도) 조회\nselect now_station, now_longitude, now_latitude\nfrom bus_view"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n-- now_station(정류소명), now_longitude(위도), now_latitude (경도) 조회\nselect now_station, now_longitude, now_latitude\nfrom bus_view\n-- now_station(정류소명), now_longitude(위도), now_latitude (경도) 를 그룹으로 그룹당 1개씩 조회\ngroup by now_station, now_longitude, now_latitude \norder by now_station -- 정류소 이름으로 정렬"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#그래프의 크기 설정 가로20 세로10\nplt.figure(figsize\u003d(20, 10))\n\n#자동으로 그래프의 모양을 맞춰줌\nplt.tight_layout()\n\n#df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n#데이터.plot(kind\u003d\"kde\") 는 비율을 그래프로 그려줌\n#도착 시간 next_arrive_time의비율을 그래프로 그림\ndf_train.toPandas()[\"next_arrive_time\"].plot(kind\u003d\u0027kde\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#그래프의 크기 설정 가로20 세로10\nplt.figure(figsize\u003d(20, 10))\n\n#자동으로 그래프의 모양을 맞춰줌\nplt.tight_layout()\n\n#df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n#데이터.plot(kind\u003d\"kde\") 는 비율을 그래프로 그려줌\n#distance (거리) 의 비율을 그래프로 그림\ndf_train.toPandas()[\"distance\"].plot(kind\u003d\u0027kde\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_train.toPandas().info()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#각 컬럼의 기울어 짐 정도 조회\n##df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n(df_train.toPandas()[numeric_column_name_list]).skew()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#그래프의 크기 설정 가로20 세로10\nplt.figure(figsize\u003d(20, 10))\n##df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n#가장 기울어진 컬럼 시각화\nplt.hist(df_train.toPandas()[\"next_arrive_time\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# df_train.withColumn(수정할 컬럼의 이름, 수정할 값)\n\n#df_train.withColumn(next_arrive_time : next_arrive_time 컬럼의 값 수정\n\n# log1p(\"next_arrive_time\") : next_arrive_time에 log(1+데이터) 를 곱한 값을 리턴하는 log1p 호출해서 컬럼갑 수정\n\ndf_train \u003d df_train.withColumn(\"next_arrive_time\", log1p(\"next_arrive_time\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#각 컬럼의 기울어 짐 정도 조회\n##df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n(df_train.toPandas()[numeric_column_name_list]).skew()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#그래프의 크기 설정 가로20 세로10\nplt.figure(figsize\u003d(20, 10))\n##df_train.toPandas() : Spark DataFrame을 Pandas DataFrame으로 변경\n#가장 기울어진 컬럼 시각화\nplt.hist(df_train.toPandas()[\"next_arrive_time\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 또 다른 기울임이 심한 컬럼 distance\nplt.figure(figsize\u003d(20, 10))\n\nplt.hist(df_train.toPandas()[\"distance\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train \u003d df_train.withColumn(\"distance\", log1p(\"distance\"))\n\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n\n#각 distance 및 next_arrive_time 을 변환한 기울임 조회\n(df_train.toPandas()[numeric_column_name_list]).skew()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 조정된 distance 시각화\nplt.figure(figsize\u003d(20, 10))\n\nplt.hist(df_train.toPandas()[\"distance\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#F.to_date(df_train[\"date\"] : date 컬럼의 타입을 String 에서 날짜 타입으로 변환\ndf_train \u003d df_train.withColumn(\u0027date\u0027,F.to_date(df_train[\"date\"]))"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train.schema"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#dayofweek(df_train[\"date\"]) : date 컬럼의 운행 날짜의 요일 리턴 (일요일 1, 월요일 2 ..)\n\n# 운행 날짜의 요일을 weekday 컬럼에 저장\ndf_train.withColumn(\"weekday\", dayofweek(df_train[\"date\"]) ).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#dayofweek(df_train[\"date\"]) : date 컬럼의 운행 날짜의 요일 리턴 (일요일 1, 월요일 2 ..)\n\n# 운행 날짜의 요일을 weekday 컬럼에 저장\n\ndf_train \u003d df_train.withColumn(\"weekday\", dayofweek(df_train[\"date\"]) )"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_train.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# SQL 사용을 위해서 Spark DataFrame을 View(데이터를 추가 수정 할 수 없는 테이블) 형태로 변환\ndf_train.createOrReplaceTempView(\u0027bus_view\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n--데이터 조회\nselect * from bus_view"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- weekday (요일 1-\u003e 일요일, 2-\u003e 월요일 ...) 별로 개수 조회\nselect weekday , count(*)\nfrom bus_view\ngroup by weekday\norder by weekday"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n--데이터 조회\nselect * from bus_view"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#분석에 사용할 독립변수 컬럼 이름 저장\nfeature_name_list \u003d [\u0027now_latitude\u0027, \u0027now_longitude\u0027,\u0027now_arrive_time\u0027, \n                     \u0027distance\u0027, \u0027next_latitude\u0027, \u0027next_longitude\u0027,  \u0027weekday\u0027\n                     ]"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfeature_name_list"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# inputCols\u003dfeature_name_list\n# feature_name_list ([\u0027now_latitude\u0027, \u0027now_longitude\u0027,\u0027now_arrive_time\u0027, \n#                     \u0027distance\u0027, \u0027next_latitude\u0027, \u0027next_longitude\u0027,  \u0027weekday\u0027])  \n#                      컬럼의 값을 합쳐서\n# outputCol\u003d\"features\" : feature 컬럼을 생성 \n# 할 객체 VectorAssembler \n\nassembler \u003d VectorAssembler(inputCols\u003dfeature_name_list, outputCol\u003d\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# feature_name_list ([\u0027now_latitude\u0027, \u0027now_longitude\u0027,\u0027now_arrive_time\u0027, \n#                     \u0027distance\u0027, \u0027next_latitude\u0027, \u0027next_longitude\u0027,  \u0027weekday\u0027])  \n#                      컬럼의 값을 합쳐서\n# outputCol\u003d\"features\" : feature 컬럼을 생성 \n\nasembler_df \u003d assembler.transform(df_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nasembler_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# asembler_df 의 데이터를 75:25 으로 나눠서\n# 75%는 trainingData 에 대입\n# 25%는 testData에 대입\n\n(trainingData, testData) \u003d asembler_df.randomSplit([0.75, 0.25])"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# trainingData 출력\ntrainingData.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# testData 조회\n\ntestData.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# RandomForestRegressor 객체 생성\n# featuresCol \u003d \"features\" : 독립변수는 features 컬럼에 저장\n# labelCol \u003d \" next_arrive_time\" : 종속 변수는  next_arrive_time 컬럼에 저장\n# numTrees\u003d100 : DecisionTree 개수 100\n\nrf \u003d RandomForestRegressor(featuresCol \u003d \"features\", labelCol \u003d \"next_arrive_time\", numTrees\u003d100)"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrf"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# trainingData를 이용해서 RandomForest에 포함된 Decision Tree들을 만듬\nrfModel \u003d rf.fit(trainingData)"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# dtModel.transform(testData) : testData 를 예측\nprediction \u003d rfModel.transform(testData)"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 예측값 조회\nprediction.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# next_arrive_time (실제값), prediction (\"예측값\") 컬럼 조회\nprediction[\"next_arrive_time\", \"prediction\"].show()"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# next_arrive_time (실제값), prediction (\"예측값\") 컬럼 조회\n\npred_label \u003d prediction.select(\"next_arrive_time\", \"prediction\")"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npred_label.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#next_arrive_time 컬럼의 로그 값을 실제 값으로 변환\n\npred_label \u003d pred_label.withColumn(\"next_arrive_time\",\n           exp(pred_label[\"next_arrive_time\"]) - 1  )"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#prediction 컬럼의 로그 값을 실제 값으로 변환\n\npred_label \u003d pred_label.withColumn(\"prediction\", \n             exp(pred_label[\"prediction\"]) - 1  )"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npred_label.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#RootMeanSquared Error 를 계산할 객체 생성\n\nmetrics \u003d RegressionMetrics(pred_label.rdd)"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#Root Mean Squared Error 계산\n#값이 낮을수록 좋음 -\u003e 오차가 적다.\n\nmetrics.rootMeanSquaredError"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#모델 저장\nrfModel.write().overwrite().save(\"/spark_rf_model\")"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}