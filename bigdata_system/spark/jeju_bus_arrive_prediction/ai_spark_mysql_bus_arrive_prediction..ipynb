{
  "metadata": {
    "name": "ai_spark_mysql_bus_arrive_prediction",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# `Spark model`, 버스 도착 시각 예측 데이터 `MySQL` 에 저장\n\n### 이전 `ai_spark_bus_arrive_prediction` 은 `Kafka topic`, `car_arrive_topic` 에 저장하였는데\n### `car_location_topic` 도 사용 중이다 보니 잘 구동되지 않는 경우(컴퓨터 메모리 부족, CPU 과부하 등의 문제 발생)가 있어\n### 보편적으로 잘 사용되기 위해 `MySQL` 에 데이터를 저장하는 파일도 생성한 것."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip uninstall pandas --y"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install -U pandas\u003d\u003d1.5.3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install ksql"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install PyMySQL"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n# pip install cffi"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom ksql import KSQLAPI\nimport pandas as pd\nimport time\nimport datetime\nimport numpy as np\nfrom pyspark.ml.regression import RandomForestRegressionModel\nfrom pyspark.ml.feature import VectorAssembler\nimport ast\nimport pymysql"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nclient \u003d KSQLAPI(\"http://localhost:8089\", timeout\u003dNone)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# MySQL DB 연결\n\nmysql_db \u003d pymysql.connect(\n        host \u003d \"192.168.0.171\" # MySQL이 설치된 윈도우 IP 주소\n        ,port\u003d3306\n        ,user\u003d\u0027root\u0027\n        ,passwd\u003d\u00271234\u0027\n        ,db\u003d\u0027bus_db\u0027\n        ,charset\u003d\u0027utf8\u0027\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nmysql_cursor \u003d mysql_db.cursor()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 하둡에 저장된 모델 읽어서 저장\n\nload_rf \u003d RandomForestRegressionModel.load(\"/spark_rf_model/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nload_rf"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfeature_name_list \u003d [\n                    \u0027now_latitude\u0027, \u0027now_longitude\u0027, \u0027now_arrive_time\u0027,\n                    \u0027distance\u0027,\u0027next_latitude\u0027,\u0027next_longitude\u0027,\u0027weekday\u0027\n                    ]"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfeature_name_list"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nget_change_table \u003d client.query(\"\"\"SELECT * FROM bus_location_topic EMIT CHANGES;\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfor i in get_change_table:\n    print(f\"수정된 테이블 조회 : {i}\")\n    \n    if \"row\" in i:\n        location \u003d ast.literal_eval(i)[0][\"row\"][\"columns\"]\n        print(f\"위치 : {location}\")\n        \n        id \u003d location[0]\n        date \u003d location[1]\n        route_id \u003d location[2]\n        vh_id \u003d location[3]\n        route_nm \u003d location[4]\n        now_latitude \u003d location[5]\n        now_longitude \u003d location[6]\n        now_station \u003d location[7]\n        now_arrive_time \u003d location[8]\n        distance \u003d location[9]\n        next_station \u003d location[10]\n        next_latitude \u003d location[11]\n        next_longitude \u003d location[12]\n        weekday \u003d location[13]\n        \n        # 예측을 위해 버스 위치 정보를 Pandas DataFrame 으로 생성\n        df_location \u003d pd.DataFrame({\n            \"now_latitude\" : [now_latitude],\n            \"now_longitude\" : [now_longitude],\n            \"now_arrive_time\" : [now_arrive_time],\n            \"distance\" : [distance],\n            \"next_latitude\" : [next_latitude],\n            \"next_longitude\" : [next_longitude],\n            \"weekday\" : [weekday]\n        })\n        \n        # Pandas DataFrame인 df_location을 Spark DataFrame 으로 변환\n        # Spark RandomForest 모델에서 예측을 위해\n        df_spark \u003d spark.createDataFrame(df_location)\n        \n        assembler \u003d VectorAssembler(inputCols\u003dfeature_name_list, outputCol\u003d\"features\")\n        \n        assembler_df \u003d assembler.transform(df_spark)\n        \n        prediction \u003d load_rf.transform(assembler_df)\n        \n        arrive_time \u003d prediction.toPandas().loc[0,\"prediction\"]\n        print(\"#\"*100)\n\n        # 학습시 예측을 로그 값으로 예측하기 때문에 np.exp(로그값) \u003d\u003e 원래 값으로 변환 출력\n        print(f\"버스로 전송할 도착 예정 시간 : {np.exp(arrive_time)}\")\n        print(\"#\"*100)\n        \n        # 다음 정류장에 도착 예정 시간을 소수 3번째 자리 반올림\n        next_arrive_time \u003d round(np.exp(arrive_time),3)\n        \n        # route_id : 노선아이디, vh_id : 버스 아이디, route_nm : 버스노선명\n        count_query \u003d f\"\"\"\n                        select count(*) from bus_arrive_prediction\n                        where route_id \u003d \u0027{route_id}\u0027\n                        and vh_id \u003d \u0027{vh_id}\u0027\n                        and route_nm \u003d \u0027{route_nm}\u0027\n                        \"\"\"\n        print(f\"count_query : {count_query}\")\n        mysql_cursor.execute(count_query)\n        \n        count \u003d mysql_cursor.fetchall()[0][0]\n        print(f\"count : {count}\")\n        \n        if count \u003c 1 : #\u003d\u003e 노선아이디, 버스아이디, 버스노선명이 일치하는 것이 없을 때 \u003d\u003d 데이터가 없을 때\n            \n            insert_query \u003d f\"\"\"INSERT INTO bus_arrive_prediction (\n                            now_date\n                            ,route_id\n                            ,vh_id\n                            ,route_nm\n                            ,now_latitude\n                            ,now_longitude\n                            ,now_station\n                            ,distance\n                            ,next_station\n                            ,next_latitude\n                            ,next_longitude\n                            ,next_arrive_time\n                            ) VALUES ( now() ,\u0027{route_id}\u0027 ,\u0027{vh_id}\u0027\n                                    ,\u0027{route_nm}\u0027,{now_latitude}\n                                    ,{now_longitude},\u0027{now_station}\u0027\n                                    ,{distance}\n                                    ,\u0027{next_station}\u0027,{next_latitude}\n                                    ,{next_longitude},{next_arrive_time}\n                        );\"\"\"\n            print(f\"insert query :\\n{insert_query}\")\n            mysql_cursor.execute(insert_query)\n            mysql_db.commit()\n        \n        else:\n            \n            update_query \u003d f\"\"\"update bus_arrive_prediction \n                        set now_date\u003dnow()\n                        ,now_latitude\u003d{now_latitude}\n                        ,now_longitude\u003d{now_longitude}\n                        ,now_station \u003d \u0027{now_station}\u0027\n                        ,distance\u003d{distance}\n                        ,next_station \u003d \u0027{next_station}\u0027\n                        ,next_latitude\u003d{next_latitude}\n                        ,next_longitude\u003d{next_longitude}\n                        ,next_arrive_time \u003d {next_arrive_time}\n                        where route_id \u003d \u0027{route_id}\u0027\n                        and vh_id \u003d \u0027{vh_id}\u0027\n                        and route_nm \u003d \u0027{route_nm}\u0027\n                        \"\"\"\n            print(f\"update query :\\n{update_query}\")\n            mysql_cursor.execute(update_query)\n            mysql_db.commit()                \n\n            \n    print(\"\u003d\"*100)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nmysql_db.close()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}